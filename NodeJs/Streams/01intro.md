What is a Stream (real meaning)

A stream is a way to process data piece-by-piece instead of loading everything into memory at once.

Instead of:

Load all data â†’ then process


Streams do:

Read chunk â†’ process â†’ next chunk


Each chunk is a Buffer ğŸ‘ˆ (connects to your previous topic)

Why Streams are needed (very important)
Without streams (bad)
const data = fs.readFileSync("big.mp4");


Problems:

Huge RAM usage

App may crash

Slow startup

Not scalable

With streams (good)
fs.createReadStream("big.mp4");


Benefits:

Constant memory usage

Faster start

Handles GB-scale files

Backpressure support

Streams mental model
Source â†’ [Buffer] â†’ [Buffer] â†’ Destination


Examples:

File â†’ HTTP response

Request â†’ File

S3 â†’ Client

Types of Streams (core concept)

Node.js has 4 types of streams.

1ï¸âƒ£ Readable Stream

Used to read data

Examples:

fs.createReadStream()

HTTP request (req)

process.stdin

Example
const fs = require("fs");

const readStream = fs.createReadStream("file.txt");

readStream.on("data", chunk => {
  console.log(chunk); // Buffer
});

readStream.on("end", () => {
  console.log("Finished reading");
});

2ï¸âƒ£ Writable Stream

Used to write data

Examples:

fs.createWriteStream()

HTTP response (res)

process.stdout

Example
const writeStream = fs.createWriteStream("out.txt");

writeStream.write("Hello ");
writeStream.write("World");
writeStream.end();

3ï¸âƒ£ Duplex Stream

Readable + Writable

Examples:

TCP sockets

WebSocket

net.Socket

Think:

Chat app
You send + receive

4ï¸âƒ£ Transform Stream

Duplex stream that modifies data

Examples:

zlib.createGzip()

Crypto streams

Custom transforms

Example
const { Transform } = require("stream");

const upperCase = new Transform({
  transform(chunk, encoding, callback) {
    callback(null, chunk.toString().toUpperCase());
  }
});

Piping in Streams (very important)
What is pipe?

Pipe connects the output of one stream directly to the input of another.

It handles:

Data flow

Backpressure

Errors (mostly)

Basic pipe example
const fs = require("fs");

fs.createReadStream("input.txt")
  .pipe(fs.createWriteStream("output.txt"));


What happens:

ReadStream â†’ Buffer â†’ WriteStream


No manual data handling needed.

Pipe with Transform
const zlib = require("zlib");

fs.createReadStream("file.txt")
  .pipe(zlib.createGzip())
  .pipe(fs.createWriteStream("file.txt.gz"));


Flow:

Read â†’ Compress â†’ Write

Why pipe is better than events

âŒ Manual:

read.on("data", chunk => write.write(chunk));


Problems:

No backpressure handling

Error-prone

Memory risk

âœ… Pipe:

Built-in backpressure

Cleaner

Safer

Backpressure (must understand)

When the writable stream is slower than the readable stream

Pipe automatically:

Pauses readable

Resumes when writable drains

Without pipe â†’ memory explosion ğŸ’¥

Real-world examples you WILL use
File upload API
req.pipe(fs.createWriteStream("upload.png"));

Streaming response
fs.createReadStream("video.mp4").pipe(res);

S3 upload (conceptually)
File Stream â†’ S3 Upload Stream

Important stream properties

highWaterMark â†’ chunk size

objectMode â†’ stream objects instead of buffers

readableFlowing â†’ flowing vs paused mode

Interview one-liner

â€œStreams allow Node.js to process data incrementally using buffers, reducing memory usage. There are four types: Readable, Writable, Duplex, and Transform, and piping connects streams efficiently with built-in backpressure handling.â€